{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文： http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translator: 陳胤泰 YinTaiChen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網路 (Neural Networks)\n",
    "===============\n",
    "\n",
    "使用``torch.nn``套組可以建造出神經網路。\n",
    "\n",
    "既然你已經對``autograd``有了初步的了解，``nn``仰賴``autograd``定義神經網路模型 (model) 並使模型各有不同。\n",
    "\n",
    "一個``nn.Module``物件包含神經網路層（layer）與回傳``output``的``forward(input)``方法。\n",
    "\n",
    "舉例來說，看看這個分類數位影像的網路：\n",
    "\n",
    "![title](http://pytorch.org/tutorials/_images/mnist.png)\n",
    "convnet\n",
    "\n",
    "這是個簡單的向前傳遞（feed-forward）網路。它會傳遞輸入給神經網路層，一層接著一層，直到最終輸出結果。\n",
    "\n",
    "訓練神經網路的典型程序如下：\n",
    "\n",
    "- 定義一個神經網路，該網路具有可學習的參數（或稱為權重）\n",
    "- 遞迴經歷一個作為輸入的資料集\n",
    "- 對輸入做運算，直到通過整個神經網路\n",
    "- 計算損失（輸出有多接近正確結果）\n",
    "- 將梯度反向傳播成神經網路的參數\n",
    "- 更新神經網路的權重值，以這個簡單的規則為代表：\n",
    "   ``權重 ＝ 權重 - 學習率 * 梯度``\n",
    "\n",
    "\n",
    "定義網路\n",
    "------------------\n",
    "讓我們定義一個像這樣的網路："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你只須定義``forward``函式。``backward``函式（梯度於此被計算）會在使用``autograd``的同時，自動被定義好。\n",
    "\n",
    "在``forward``函式中，你可以使用任何的張量運算。\n",
    "\n",
    "模型可學習的參數由``net.parameters()``函式回傳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要向前傳遞的輸入是個``autograd.Variable``物件，輸出也是。\n",
    "\n",
    "這個網路（LeNet）預期的輸入大小為 32x32。\n",
    "\n",
    "注意：若要將這個網路用在 MNIST 資料集，請將該資料集的影像大小縮放至 32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0132 -0.0327 -0.0120  0.0312  0.0905 -0.0440  0.0449 -0.0883 -0.0292  0.1113\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將所有參數的梯度暫存區 (gradient buffer) 歸零，並以隨機梯度做反向傳播："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意事項</h4><p>torch.nn套組只支援小批次學習（mini-batch，亦即它只接受小批的資料樣本（sample），而非單一資料樣本。\n",
    "    \n",
    "    舉例來說，``nn.Conv2d``會接收4D張量，其維度為：``樣本數 x 頻道數 x 高度 x 寬度``。\n",
    "\n",
    "    如果你有一個單獨的資料樣本，只須使用``input.unsqueeze(0)``函式來添加一個假的批次維度（batch dimension）。</p></div>\n",
    "\n",
    "在更進一步之前，讓我們複習一下到目前為止見過的所有類別。\n",
    "\n",
    "**複習:**\n",
    "  -  ``torch.Tensor`` - 『多維陣列』的類別。\n",
    "  -  ``autograd.Variable`` - 將張量包裹起來，並紀錄對其所做的所有運算。與``Tensor``有相同的應用程式界面(API)。它也具有與張量相關聯的的梯度。\n",
    "  -  ``nn.Module`` - 神經網路模組。是個封裝(encapsulating)參數的簡便方法，並幫助使用者將參數移動到GPU、匯出、匯入等等。\n",
    "  -  ``nn.Parameter`` - Variable 的一種，唯其在被指派為（assigned）``Module``的屬性時，被自動登記為參數。\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - 實作 autograd 運算向前與向後的定義。每一個``Variable``運算會創建至少一個``Function``節點(node)。該節點與創建``Variable``的函式相連，並編碼了``Variable``的創建歷史。\n",
    "\n",
    "**至此，我們涵蓋了：**\n",
    "  -  定義一個神經網路\n",
    "  -  對輸入做運算並實施反向傳播\n",
    "\n",
    "**還剩下:**\n",
    "  -  計算損失\n",
    "  -  更新神經網路的權重\n",
    "\n",
    "損失函數 (Loss Function)\n",
    "-------------\n",
    "損失函數接收（輸出值, 目標值）這樣一對數值作為它的輸入，並計算出一個數值，用來估計輸出值與目標值相距多遠。\n",
    "\n",
    "在 nn 套組之中，有許多不同的`loss function`。\n",
    "\n",
    "``nn.MSELoss``是個簡單的例子。它計算輸出值與目標值之間的平均方差(mean-squared error)。\n",
    "\n",
    "舉例來說:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.3670\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在呢，如果你循著反向傳播的方向，並使用``loss``的``.grad_fn``屬性來追蹤它，你會看到像這樣一張計算圖(graph of computations)：\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "當我們呼叫``loss.backward()``，整張圖將對 loss 做微分。圖中所有的 Variable 將得到``.grad``這個 Variable，並隨著梯度累積。\n",
    "\n",
    "為了具體的了解，讓我們追蹤幾個反向傳播的步驟："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f99046e7cc0>\n",
      "<AddmmBackward object at 0x7f99046e7c88>\n",
      "<ExpandBackward object at 0x7f99046e7cc0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向傳播\n",
    "--------\n",
    "要反向傳播誤差，我們只需呼叫``loss.backward()``。\n",
    "\n",
    "然而，你得清除既存的梯度，否則計算出來的梯度會累積它之上。\n",
    "\n",
    "如此我們便能呼叫``loss.backward()``，並看看第一層卷積層之偏權值(bias)的梯度，在反向傳播之前與之後的變化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.1114\n",
      "-0.0492\n",
      " 0.0608\n",
      "-0.0078\n",
      " 0.0347\n",
      "-0.0627\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我們了解到該如何使用損失函數。\n",
    "\n",
    "**稍後閱讀:**\n",
    "\n",
    "  與神經網路相關的套組具有各式各樣的模組與損失函數，這兩者形成了建構深層神經網路(deep neural networks)的模塊。\n",
    "  \n",
    "  完整的清單與說明文件在此：http://pytorch.org/docs/nn\n",
    "\n",
    "**只剩一件事還沒學:**\n",
    "\n",
    "  - 更新神經網路的權重\n",
    "\n",
    "更新權重\n",
    "------------------\n",
    "在實際應用上，更新權重最簡單的規則是梯度隨機下降法(Stochastic Gradient Descent; SGD)：\n",
    "\n",
    "     梯度 = 梯度 - 學習率 * 梯度\n",
    "\n",
    "我們可以用簡單幾行 Python 程式碼，將這條規則實作出來。\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "然而，在使用神經網路時，你會想要使用各式各樣的更新規則，像是SGD、Nesterov-SGD、Adam、RMSProp，等等。\n",
    "\n",
    "為了讓這件事成真，我們建造了一個小套組：``torch.optim``。\n",
    "\n",
    "它實作了上述所有方法。使用它是很簡單的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
